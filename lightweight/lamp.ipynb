{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84372605 0.86577179 1.301581   0.57883034 1.05201817 3.31532217]\n",
      "0.9161263956256981\n",
      "[194, 197, 255, 159, 222, 0]\n",
      "1\n",
      "I had had a feeling of freedom because of the sudden change in my life.\n",
      "sentence + word 50.0 3.7333333333333334\n",
      "adj 0.14084507042253522\n",
      "final array 0.843726051161871\n",
      "sentence_length 50.0\n",
      "sentiment of sentence 0.0\n",
      "194\n",
      "         \n",
      "2\n",
      "By comparison to what had come before, I felt immensely free.\n",
      "sentence + word 36.666666666666664 4.454545454545454\n",
      "adj 0.3278688524590164\n",
      "final array 0.8657717911367057\n",
      "sentence_length 36.666666666666664\n",
      "sentiment of sentence 40.0\n",
      "197\n",
      "         \n",
      "3\n",
      "But then, once I became used to that freedom, even small tasks became more difficult.\n",
      "sentence + word 50.0 4.533333333333333\n",
      "adj 0.5882352941176471\n",
      "final array 1.3015810040987597\n",
      "sentence_length 50.0\n",
      "sentiment of sentence -8.333333333333332\n",
      "255\n",
      "         \n",
      "4\n",
      "I placed constraints on myself, and filled the hours of the day.\n",
      "sentence + word 40.0 4.25\n",
      "adj 0.0\n",
      "final array 0.5788303361857744\n",
      "sentence_length 40.0\n",
      "sentiment of sentence 40.0\n",
      "159\n",
      "         \n",
      "5\n",
      "Or perhaps it was even more complicated than that.\n",
      "sentence + word 30.0 4.555555555555555\n",
      "adj 0.6\n",
      "final array 1.0520181658600773\n",
      "sentence_length 30.0\n",
      "sentiment of sentence 0.0\n",
      "222\n",
      "         \n",
      "6\n",
      "Sometimes I did exactly what I wanted to do all day—I lay on the sofa and read a book, or I typed up an old diary—and then the most terrifying sort of despair would descend on me: the very freedom I was enjoying seemed to say that what I did in my day was arbitrary, and that therefore my whole life and how I spent it was arbitrary.\n",
      "sentence + word 226.66666666666666 3.823529411764706\n",
      "adj 0.3003003003003003\n",
      "final array 3.315322168856311\n",
      "sentence_length 226.66666666666666\n",
      "sentiment of sentence 4.500000000000002\n",
      "0\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# NEED TO NORAMLIZE EVERYTHING TO WEIGHT IT PROPERLY\n",
    "\n",
    "\n",
    "# loads the text and formats it so that it is a list of sentences (and each sentence is a list of words)\n",
    "# data is just the sentence tokenized\n",
    "def load_corpus(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            line = line.replace('\\n', ' ')\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "        \n",
    "    return data, tokenized_data\n",
    "\n",
    "# IF I want to parse poetry for enjambment\n",
    "def load_poem(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "    return data, tokenized_data\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    \n",
    "    \n",
    "    filtered = []\n",
    "    removed_count= []\n",
    "    for sentence in corpus:\n",
    "        sent = []\n",
    "        removed = 0\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                sent.append(word)\n",
    "            else:\n",
    "                removed += 1\n",
    "        removed_count.append(removed)\n",
    "        \n",
    "        filtered.append(sent)\n",
    "    return filtered, removed\n",
    "\n",
    "# takes in corpus and returns a numpy array that has the length of each sentence\n",
    "def get_sentence_len(corpus):\n",
    "    arr = []\n",
    "    for sentence in corpus:\n",
    "        arr.append(len(sentence))\n",
    "    return np.array(arr)\n",
    "\n",
    "# takes in numpy array of sentences and returns a normalized numpy array\n",
    "def process_sentence_length(sentences):\n",
    "    max_len = np.max(sentences)\n",
    "    sentences = sentences / 30 # our max\n",
    "    return sentences * 100\n",
    "    \n",
    "def get_sentiment(corpus):\n",
    "    sentiment_array = []\n",
    "    for sentence in corpus:\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_array.append(blob.sentiment.polarity)\n",
    "    sentiment_array = np.array(sentiment_array)\n",
    "    return sentiment_array * 100\n",
    "\n",
    "# gets a sentence, returns an average length of the words\n",
    "def word_length(sentence):\n",
    "    length = 0\n",
    "    for word in sentence:\n",
    "        length += len(word)\n",
    "    length = float(length)\n",
    "    avg = length / len(sentence)\n",
    "    return avg\n",
    "\n",
    "# counts the adjectives in a sentence and returns the percentage\n",
    "# of the sentence that is an adjective\n",
    "def adj_count(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tags = sent.tags\n",
    "    count = 0\n",
    "    for t in tags:\n",
    "        if t[1] == \"JJ\" or t[1] == \"RB\":\n",
    "            count += 1\n",
    "    count = float(count)\n",
    "    return count/len(sentence)\n",
    "\n",
    "# pass in a numpy array, divides it by its maximum and multiplied by 100\n",
    "def normalize_adj(adj_array):\n",
    "    return adj_array * 10 # 1000 is normal for this, then divide by half because waited less (it's waited like 10%)\n",
    "    \n",
    "\n",
    "def rgb_to_hex(a):\n",
    "    result = '#%02x%02x%02x' % (a,a,a)\n",
    "#     print(result)\n",
    "    return result\n",
    "    \n",
    "def main(data_dir):\n",
    "    # gets the data in the formatting we want\n",
    "    sentence_lists, corpus = load_corpus(data_dir)\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    sentence_lengths = get_sentence_len(corpus)\n",
    "    sentence_lengths = process_sentence_length(sentence_lengths)\n",
    "    \n",
    "    sentiment_of_sentence = get_sentiment(sentence_lists)\n",
    "    \n",
    "    weight = []\n",
    "\n",
    "    \n",
    "    word_lengths = []\n",
    "    adjective_count = []\n",
    "\n",
    "    for sentence in sentence_lists:\n",
    "        adjective_count.append(adj_count(sentence))\n",
    "    for sentence in corpus:\n",
    "        word_lengths.append(word_length(sentence))\n",
    "        \n",
    "    word_lengths = np.array(word_lengths)\n",
    "    adjective_count = np.array(adjective_count)\n",
    "    \n",
    "    adjective_count = normalize_adj(adjective_count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_arr = np.add(sentence_lengths, word_lengths)\n",
    "        \n",
    "    final_arr = (final_arr/np.average(final_arr)) + adjective_count\n",
    "    \n",
    "    max_num = np.max(final_arr)\n",
    "    min_num = np.min(final_arr)\n",
    "    \n",
    "    \n",
    "    final_list = final_arr.tolist()\n",
    "    \n",
    "    # scales the list into colors from 0 to 255 for rgb\n",
    "    print(final_arr)\n",
    "    avg = np.average(final_arr)\n",
    "    \n",
    "    sd = np.std(final_arr)\n",
    "    print(sd)\n",
    "    \n",
    "    # weird thing i am trying to inflate the higher ones\n",
    "    for i in range(0, len(final_list)):\n",
    "        if final_list[i] > (avg - sd) and final_list[i] < (max_num - sd):\n",
    "            more_than = final_list[i]  / avg  # thisi s a number greater than one    \n",
    "#             print(final_list[i])\n",
    "            final_list[i] =  (1  +  ( sd * more_than) )\n",
    "#             print(final_list[i])\n",
    "#             print(\"---\")\n",
    "        elif final_list[i] > 3*sd:\n",
    "            final_list[i] -= 3*sd\n",
    "            \n",
    "    nump = np.array(final_list)\n",
    "    new_min = np.min(nump)\n",
    "    new_max = np.max(nump)\n",
    "                   \n",
    "    color_list = []\n",
    "    for num in final_list:\n",
    "        x_i = (num - new_min) / (new_max - new_min)\n",
    "        color_list.append(int(x_i * 255))\n",
    "        \n",
    "    print(color_list)\n",
    "        \n",
    "\n",
    "\n",
    "    for x in range(0, len(sentence_lists)):\n",
    "        print(x+1)\n",
    "        print(sentence_lists[x])\n",
    "        print(\"sentence + word\", sentence_lengths[x], word_lengths[x])\n",
    "        print(\"adj\", adjective_count[x])\n",
    "        print(\"final array\", final_arr[x])\n",
    "        print(\"sentence_length\", sentence_lengths[x])\n",
    "        print(\"sentiment of sentence\", sentiment_of_sentence[x])\n",
    "        print(color_list[x])\n",
    "        print(\"         \")\n",
    "    \n",
    "#     print(color_list)\n",
    "\n",
    "    # convert everything to hex\n",
    "    for c in range(0, len(color_list)):\n",
    "        color_list[c] = rgb_to_hex(color_list[c])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # formula is (x_i - min(x) / max(x) - min(x))\n",
    "    # to combine: take average of the sentence and word length\n",
    "    # add on the adjective count with a norm of\n",
    "\n",
    "    with open('colors/cant_and_wont.json', 'w') as outfile:\n",
    "        json.dump(color_list, outfile, indent=4)\n",
    "\n",
    "\n",
    "    \n",
    "# to manually specify the path to the data.\n",
    "# This may take a little bit of time (~30-60 seconds) to run.\n",
    "if __name__ == '__main__':\n",
    "    data_dir = 'text/cant_and_wont.txt'\n",
    "    main(data_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Mankind, ignorant of the truths that lie within every human being, looked outward—pushed ever outward.\n",
      "1 What mankind hoped to learn in its outward push was who was actually in charge of all creation, and what all creation was all about.\n",
      "2 Mankind flung its advance agents ever outward, ever outward.\n",
      "3 Eventually it flung them out into space, into the colorless, tasteless, weightless sea of outwardness without end.\n",
      "4 It flung them like stones.\n",
      "5 These unhappy agents found that what had already been found in abundance on Earth—a nightmare of meaninglessness without end.\n",
      "6 The bounties of space, of infinite outwardness, were three: empty heroics, low comedy, and pointless death.\n",
      "7 Outwardness lost, at last, its imagined attractions.\n",
      "8 Only inwardness remained to be explored.\n",
      "9 Only the human soul remain terra incognita.\n",
      "10 This was the beginning of goodness and wisdom.\n",
      "[1.50665861 1.98558229 1.44207256 1.53182876 0.45506633 1.66171125\n",
      " 1.65416018 1.02744072 0.80974613 1.31869146 0.6851438 ]\n",
      "0.45180406951996105\n",
      "[253, 68, 248, 255, 32, 1, 0, 218, 104, 239, 79]\n",
      "['#fdfdfd', '#444444', '#f8f8f8', '#ffffff', '#202020', '#010101', '#000000', '#dadada', '#686868', '#efefef', '#4f4f4f']\n",
      "[4, 16, 1, 7, 2, 8, 5, 2, 3, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# NEED TO NORAMLIZE EVERYTHING TO WEIGHT IT PROPERLY\n",
    "\n",
    "\n",
    "# loads the text and formats it so that it is a list of sentences (and each sentence is a list of words)\n",
    "# data is just the sentence tokenized\n",
    "def load_corpus(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            line = line.replace('\\n', ' ')\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "        \n",
    "    return data, tokenized_data\n",
    "\n",
    "# IF I want to parse poetry for enjambment\n",
    "def load_poem(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "    return data, tokenized_data\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    filtered = []\n",
    "    removed_count= []\n",
    "    for sentence in corpus:\n",
    "        sent = []\n",
    "        removed = 0\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                sent.append(word)\n",
    "            else:\n",
    "                removed += 1\n",
    "        removed_count.append(removed)\n",
    "        \n",
    "        filtered.append(sent)\n",
    "    return filtered, removed_count\n",
    "\n",
    "# takes in corpus and returns a numpy array that has the length of each sentence\n",
    "def get_sentence_len(corpus):\n",
    "    arr = []\n",
    "    for sentence in corpus:\n",
    "        arr.append(len(sentence))\n",
    "    return np.array(arr)\n",
    "\n",
    "# takes in numpy array of sentences and returns a normalized numpy array\n",
    "def process_sentence_length(sentences):\n",
    "    max_len = np.max(sentences)\n",
    "    sentences = sentences / 30 # our max\n",
    "    return sentences * 100\n",
    "    \n",
    "def get_sentiment(corpus):\n",
    "    sentiment_array = []\n",
    "    for sentence in corpus:\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_array.append(blob.sentiment.polarity)\n",
    "    sentiment_array = np.array(sentiment_array)\n",
    "    return sentiment_array * 100\n",
    "\n",
    "# gets a sentence, returns an average length of the words\n",
    "def word_length(sentence):\n",
    "    length = 0\n",
    "    for word in sentence:\n",
    "        length += len(word)\n",
    "    length = float(length)\n",
    "    avg = length / len(sentence)\n",
    "    return avg\n",
    "\n",
    "# counts the adjectives in a sentence and returns the percentage\n",
    "# of the sentence that is an adjective\n",
    "def adj_count(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tags = sent.tags\n",
    "    count = 0\n",
    "    for t in tags:\n",
    "        if t[1] == \"JJ\" or t[1] == \"RB\":\n",
    "            count += 1\n",
    "    count = float(count)\n",
    "    return count/len(sentence)\n",
    "\n",
    "# pass in a numpy array, divides it by its maximum and multiplied by 100\n",
    "def normalize_adj(adj_array):\n",
    "    return adj_array * 10 # 1000 is normal for this, then divide by half because waited less (it's waited like 10%)\n",
    "    \n",
    "\n",
    "def rgb_to_hex(a):\n",
    "    result = '#%02x%02x%02x' % (a,a,a)\n",
    "#     print(result)\n",
    "    return result\n",
    "    \n",
    "def main(data_dir):\n",
    "    # gets the data in the formatting we want\n",
    "    sentence_lists, corpus = load_corpus(data_dir)\n",
    "    \n",
    "    for c in range(len(sentence_lists)):\n",
    "        print(c, sentence_lists[c])\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    sentence_lengths = get_sentence_len(corpus)\n",
    "    sentence_lengths = process_sentence_length(sentence_lengths)\n",
    "    \n",
    "    sentiment_of_sentence = get_sentiment(sentence_lists)\n",
    "    \n",
    "    weight = []\n",
    "\n",
    "    \n",
    "    word_lengths = []\n",
    "    adjective_count = []\n",
    "\n",
    "    for sentence in sentence_lists:\n",
    "        adjective_count.append(adj_count(sentence))\n",
    "    for sentence in corpus:\n",
    "        word_lengths.append(word_length(sentence))\n",
    "        \n",
    "    word_lengths = np.array(word_lengths)\n",
    "    adjective_count = np.array(adjective_count)\n",
    "    \n",
    "    adjective_count = normalize_adj(adjective_count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_arr = np.add(sentence_lengths, word_lengths)\n",
    "        \n",
    "    final_arr = (final_arr/np.average(final_arr)) + adjective_count\n",
    "    \n",
    "    max_num = np.max(final_arr)\n",
    "    min_num = np.min(final_arr)\n",
    "    \n",
    "    \n",
    "    final_list = final_arr.tolist()\n",
    "    \n",
    "    # scales the list into colors from 0 to 255 for rgb\n",
    "    print(final_arr)\n",
    "    avg = np.average(final_arr)\n",
    "    \n",
    "    sd = np.std(final_arr)\n",
    "    print(sd)\n",
    "    \n",
    "    # weird thing i am trying to inflate the higher ones\n",
    "    for i in range(0, len(final_list)):\n",
    "        if final_list[i] > (avg - sd) and final_list[i] < (max_num - sd):\n",
    "            more_than = final_list[i]  / avg  # thisi s a number greater than one    \n",
    "#             print(final_list[i])\n",
    "            final_list[i] =  (1  +  ( sd * more_than) )\n",
    "#             print(final_list[i])\n",
    "#             print(\"---\")\n",
    "        elif final_list[i] > 3*sd:\n",
    "            final_list[i] -= 3*sd\n",
    "            \n",
    "    nump = np.array(final_list)\n",
    "    new_min = np.min(nump)\n",
    "    new_max = np.max(nump)\n",
    "                   \n",
    "    color_list = []\n",
    "    for num in final_list:\n",
    "        x_i = (num - new_min) / (new_max - new_min)\n",
    "        color_list.append(int(x_i * 255))\n",
    "        \n",
    "    print(color_list)\n",
    "        \n",
    "    \n",
    "#     print(color_list)\n",
    "\n",
    "    # convert everything to hex\n",
    "    for c in range(0, len(color_list)):\n",
    "        color_list[c] = rgb_to_hex(color_list[c])\n",
    "        \n",
    "        \n",
    "\n",
    "    ##\n",
    "    print(color_list)\n",
    "    corpus, removed_count = remove_stopwords(corpus)\n",
    "    \n",
    "    dict_count = {}\n",
    "    \n",
    "    print(removed_count)\n",
    "\n",
    "    for c in range(0, len(color_list)):\n",
    "        smol_dict = {}\n",
    "        smol_dict[\"removed\"] = removed_count[c];\n",
    "        smol_dict[\"color\"] =  color_list[c];\n",
    "        dict_count[int(c)] = (smol_dict)\n",
    "        \n",
    "    \n",
    "    # formula is (x_i - min(x) / max(x) - min(x))\n",
    "    # to combine: take average of the sentence and word length\n",
    "    # add on the adjective count with a norm of\n",
    "\n",
    "    with open('colors/vonnegut.json', 'w') as outfile:\n",
    "        json.dump(dict_count, outfile, indent=4)\n",
    "\n",
    "\n",
    "    \n",
    "# to manually specify the path to the data.\n",
    "# This may take a little bit of time (~30-60 seconds) to run.\n",
    "if __name__ == '__main__':\n",
    "    data_dir = 'text/vonnegut_sirens_of_titan.txt'\n",
    "    main(data_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I had had a feeling of freedom because of the sudden change in my life.\n",
      "1 By comparison to what had come before, I felt immensely free.\n",
      "2 But then, once I became used to that freedom, even small tasks became more difficult.\n",
      "3 I placed constraints on myself, and filled the hours of the day.\n",
      "4 Or perhaps it was even more complicated than that.\n",
      "5 Sometimes I did exactly what I wanted to do all day—I lay on the sofa and read a book, or I typed up an old diary—and then the most terrifying sort of despair would descend on me: the very freedom I was enjoying seemed to say that what I did in my day was arbitrary, and that therefore my whole life and how I spent it was arbitrary.\n",
      "[0.70288098 0.53790294 0.71334571 0.57883034 0.45201817 3.01502187]\n",
      "0.9057193452995492\n",
      "[24, 8, 26, 12, 0, 255]\n",
      "[4 1 3 0 2 5]\n",
      "['#000000', '#080808', '#0c0c0c', '#181818', '#1a1a1a', '#ffffff']\n",
      "Or perhaps it was even more complicated than that.\n",
      "By comparison to what had come before, I felt immensely free.\n",
      "I placed constraints on myself, and filled the hours of the day.\n",
      "I had had a feeling of freedom because of the sudden change in my life.\n",
      "But then, once I became used to that freedom, even small tasks became more difficult.\n",
      "Sometimes I did exactly what I wanted to do all day—I lay on the sofa and read a book, or I typed up an old diary—and then the most terrifying sort of despair would descend on me: the very freedom I was enjoying seemed to say that what I did in my day was arbitrary, and that therefore my whole life and how I spent it was arbitrary.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# NEED TO NORAMLIZE EVERYTHING TO WEIGHT IT PROPERLY\n",
    "\n",
    "\n",
    "# loads the text and formats it so that it is a list of sentences (and each sentence is a list of words)\n",
    "# data is just the sentence tokenized\n",
    "def load_corpus(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            line = line.replace('\\n', ' ')\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "        \n",
    "    return data, tokenized_data\n",
    "\n",
    "# IF I want to parse poetry for enjambment\n",
    "def load_poem(path):\n",
    "    with open(path, \"r\",encoding='utf-8') as f:\n",
    "        data = \"\"\n",
    "        for line in f:\n",
    "            data += line\n",
    "    data = sent_tokenize(data)\n",
    "    tokenized_data = []\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'[^\\w\\s]','', sent)\n",
    "        sent = sent.lower()\n",
    "        word = word_tokenize(sent)\n",
    "        \n",
    "        if word != '.':\n",
    "            tokenized_data.append(word)\n",
    "    return data, tokenized_data\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    filtered = []\n",
    "    removed_count= []\n",
    "    for sentence in corpus:\n",
    "        sent = []\n",
    "        removed = 0\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                sent.append(word)\n",
    "            else:\n",
    "                removed += 1\n",
    "        removed_count.append(removed)\n",
    "        \n",
    "        filtered.append(sent)\n",
    "    return filtered, removed_count\n",
    "\n",
    "# takes in corpus and returns a numpy array that has the length of each sentence\n",
    "def get_sentence_len(corpus):\n",
    "    arr = []\n",
    "    for sentence in corpus:\n",
    "        arr.append(len(sentence))\n",
    "    return np.array(arr)\n",
    "\n",
    "# takes in numpy array of sentences and returns a normalized numpy array\n",
    "def process_sentence_length(sentences):\n",
    "    max_len = np.max(sentences)\n",
    "    sentences = sentences / 30 # our max\n",
    "    return sentences * 100\n",
    "    \n",
    "def get_sentiment(corpus):\n",
    "    sentiment_array = []\n",
    "    for sentence in corpus:\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_array.append(blob.sentiment.polarity)\n",
    "    sentiment_array = np.array(sentiment_array)\n",
    "    return sentiment_array * 100\n",
    "\n",
    "# gets a sentence, returns an average length of the words\n",
    "def word_length(sentence):\n",
    "    length = 0\n",
    "    for word in sentence:\n",
    "        length += len(word)\n",
    "    length = float(length)\n",
    "    avg = length / len(sentence)\n",
    "    return avg\n",
    "\n",
    "# counts the adjectives in a sentence and returns the percentage\n",
    "# of the sentence that is an adjective\n",
    "def adj_count(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tags = sent.tags\n",
    "    count = 0\n",
    "    for t in tags:\n",
    "        if t[1] == \"JJ\" or t[1] == \"RB\":\n",
    "            count += 1\n",
    "    count = float(count)\n",
    "    return count/len(sentence)\n",
    "\n",
    "# pass in a numpy array, divides it by its maximum and multiplied by 100\n",
    "def normalize_adj(adj_array):\n",
    "    return adj_array * 10 # 1000 is normal for this, then divide by half because waited less (it's waited like 10%)\n",
    "    \n",
    "\n",
    "def rgb_to_hex(a):\n",
    "    result = '#%02x%02x%02x' % (a,a,a)\n",
    "#     print(result)\n",
    "    return result\n",
    "    \n",
    "def main(data_dir):\n",
    "    # gets the data in the formatting we want\n",
    "    sentence_lists, corpus = load_corpus(data_dir)\n",
    "    \n",
    "    for c in range(len(sentence_lists)):\n",
    "        print(c, sentence_lists[c])\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    sentence_lengths = get_sentence_len(corpus)\n",
    "    sentence_lengths = process_sentence_length(sentence_lengths)\n",
    "    \n",
    "#     sentiment_of_sentence = get_sentiment(sentence_lists)\n",
    "    \n",
    "    weight = []\n",
    "\n",
    "    \n",
    "    word_lengths = []\n",
    "#     adjective_count = []\n",
    "\n",
    "#     for sentence in sentence_lists:\n",
    "#         adjective_count.append(adj_count(sentence))\n",
    "    for sentence in corpus:\n",
    "        word_lengths.append(word_length(sentence))\n",
    "        \n",
    "    word_lengths = np.array(word_lengths)\n",
    "#     adjective_count = np.array(adjective_count)\n",
    "    \n",
    "#     adjective_count = normalize_adj(adjective_count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_arr = np.add(sentence_lengths, word_lengths)\n",
    "        \n",
    "    final_arr = (final_arr/np.average(final_arr))\n",
    "    \n",
    "    max_num = np.max(final_arr)\n",
    "    min_num = np.min(final_arr)\n",
    "    \n",
    "    \n",
    "    final_list = final_arr.tolist()\n",
    "    \n",
    "    # scales the list into colors from 0 to 255 for rgb\n",
    "    print(final_arr)\n",
    "    avg = np.average(final_arr)\n",
    "    \n",
    "    sd = np.std(final_arr)\n",
    "    print(sd)\n",
    "    \n",
    "    # weird thing i am trying to inflate the higher ones\n",
    "#     for i in range(0, len(final_list)):\n",
    "#         if final_list[i] > (avg - sd) and final_list[i] < (max_num - sd):\n",
    "#             more_than = final_list[i]  / avg  # thisi s a number greater than one    \n",
    "# #             print(final_list[i])\n",
    "#             final_list[i] =  (1  +  ( sd * more_than) )\n",
    "# #             print(final_list[i])\n",
    "# #             print(\"---\")\n",
    "#         elif final_list[i] > 3*sd:\n",
    "#             final_list[i] -= 3*sd\n",
    "            \n",
    "    nump = np.array(final_list)\n",
    "    new_min = np.min(nump)\n",
    "    new_max = np.max(nump)\n",
    "                   \n",
    "    color_list = []\n",
    "    for num in final_list:\n",
    "        x_i = (num - new_min) / (new_max - new_min)\n",
    "        color_list.append(int(x_i * 255))\n",
    "        \n",
    "    print(color_list)\n",
    "    \n",
    "    color_np = np.array(color_list)\n",
    "    args = np.argsort(color_np)\n",
    "    \n",
    "    print(args)\n",
    "    \n",
    "    color_list= np.sort(np.array(color_list)).tolist()\n",
    "        \n",
    "    \n",
    "#     print(color_list)\n",
    "\n",
    "    # convert everything to hex\n",
    "    for c in range(0, len(color_list)):\n",
    "        color_list[c] = rgb_to_hex(color_list[c])\n",
    "        \n",
    "        \n",
    "\n",
    "    ##\n",
    "    print(color_list)\n",
    "    \n",
    "    for number in args:\n",
    "        print(sentence_lists[number])\n",
    "\n",
    "    \n",
    "    # add on the adjective count with a norm of\n",
    "\n",
    "    with open('colors/cant_and_wont.json', 'w') as outfile:\n",
    "        json.dump(color_list, outfile, indent=4)\n",
    "\n",
    "\n",
    "    \n",
    "# to manually specify the path to the data.\n",
    "# This may take a little bit of time (~30-60 seconds) to run.\n",
    "if __name__ == '__main__':\n",
    "    data_dir = 'text/cant_and_wont.txt'\n",
    "    main(data_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
